{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "https://github.com/karthik111/video_anomaly_detection/blob/master/notebooks/VIT_Feature_Extractor.ipynb",
      "authorship_tag": "ABX9TyNUMjf5ul7HUNzV9W261pZo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthik111/video_anomaly_detection/blob/master/notebooks/VIT_Feature_Extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install decord"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G76Jew5cMpMZ",
        "outputId": "2c8b0f55-7966-4306-c167-40afbb4887a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: decord in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FGVvpP-5CayC"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTImageProcessor, ViTModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "url_c = 'https://farm4.staticflickr.com/3545/3409800178_24c6f790e6_z.jpg'\n",
        "image_c = Image.open(requests.get(url_c, stream=True).raw)\n",
        "\n",
        "url_d = 'https://farm6.staticflickr.com/5332/9374828651_07f9433075_z.jpg'\n",
        "image_d = Image.open(requests.get(url_d, stream=True).raw)\n",
        "\n",
        "url_d1 = 'https://farm3.staticflickr.com/2556/4228514131_81f3416db3_z.jpg'\n",
        "image_d1 = Image.open(requests.get(url_d1, stream=True).raw)\n",
        "\n",
        "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "#image = [np.random.randn(3, 224, 224) for _ in range(10)]\n",
        "\n",
        "inputs = processor(images=[image, image_c, image_d, image_d1], return_tensors=\"pt\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "\n",
        "last_hidden_states = outputs.last_hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['pixel_values'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBxpo50hCrMu",
        "outputId": "7dd23055-9e9f-4f6a-f252-f45131b92ed9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_states.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrp-jsTkDDPH",
        "outputId": "217b80f8-5ed6-4a8b-982b-eb95fb40e14d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = ['/content/drive/MyDrive/colab/data/kart.mov',\n",
        "              '/content/drive/MyDrive/colab/data/Assault007_x264.mp4',\n",
        "              '/content/drive/MyDrive/colab/data/Explosion002_x264.mp4']\n"
      ],
      "metadata": {
        "id": "ZAeI0b8lJ9F-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import decord\n",
        "vr = decord.VideoReader(video_path[2])"
      ],
      "metadata": {
        "id": "8sMA5kI3EzqN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vr.get_avg_fps()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d48m1tUhMk3E",
        "outputId": "85129022-57e5-40a0-802e-2f6cf336b60a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30.0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total number of frames in the video\n",
        "num_frames = len(vr)\n",
        "\n",
        "# Read all frames\n",
        "frames = [vr[i].asnumpy() for i in range(num_frames)]\n",
        "\n",
        "# Display some information\n",
        "print(f\"Total number of frames: {num_frames}\")\n",
        "print(f\"Shape of a single frame: {frames[0].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7CS0-fdMlxL",
        "outputId": "a964c4f9-51db-4123-9720-e88a4af1aab7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of frames: 4013\n",
            "Shape of a single frame: (240, 320, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jOQpF-E-9y91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(images=frames[:32], return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "YDpDcxujE-qv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['pixel_values'].shape"
      ],
      "metadata": {
        "id": "5KB4fEX6FAQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58b73a2-2ac8-4ca5-9018-1140e8091530"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "last_hidden_states = outputs.last_hidden_state"
      ],
      "metadata": {
        "id": "Fn5UJuh2OKWd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_states.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYg2x2osOcj5",
        "outputId": "f0aac31d-3683-4759-8488-bafbe22c1c12"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: flatten above [32,197,168] vector to [32,192*168] vector\n",
        "\n",
        "flattened_vector = last_hidden_states.reshape(last_hidden_states.shape[0], -1)\n",
        "flattened_vector.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgQnRylcO4iE",
        "outputId": "0fbe4b58-4fd9-40f9-fd7a-006b7baff71b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 151296])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Start the timer\n",
        "start_time_a = time.time()\n",
        "\n",
        "# prompt: call outputs = model(**inputs) for each consecutive 32 sequence of frames within frames\n",
        "base_name = video_path[2]\n",
        "import numpy as np\n",
        "frames_split = np.array_split(frames, 32)\n",
        "i = 0\n",
        "outputs_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for frames_batch in frames_split:\n",
        "    start_time_b = time.time()\n",
        "    inputs = processor(images=frames_batch, return_tensors=\"pt\")\n",
        "    start_time_b = time.time()\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**inputs)\n",
        "    end_time_b = time.time()\n",
        "    elapsed_time = end_time_b - start_time_b\n",
        "    print(f\"Elapsed time: Segment {i} {elapsed_time:.2f} seconds\")\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    flattened_vector = last_hidden_states.reshape(last_hidden_states.shape[0], -1)\n",
        "    outputs_list.append(flattened_vector)\n",
        "\n",
        "    # Define the file name with the segment index\n",
        "    file_name = f\"{base_name}_{i}.pt\"\n",
        "    tensor_segment = torch.tensor(flattened_vector)\n",
        "    # Save the tensor to a file\n",
        "    torch.save(tensor_segment, file_name)\n",
        "\n",
        "    print(f\"Saved {file_name}\")\n",
        "    i += 1\n",
        "\n",
        "end_time_a = time.time()\n",
        "elapsed_time = end_time_a - start_time_a\n",
        "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "OjhBlYTa35Un",
        "outputId": "bbc92be6-934d-4814-ad87-52558b607cea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: Segment 0 12.49 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_0.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-e975e65b086f>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tensor_segment = torch.tensor(flattened_vector)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: Segment 1 12.76 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_1.pt\n",
            "Elapsed time: Segment 2 12.94 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_2.pt\n",
            "Elapsed time: Segment 3 13.02 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_3.pt\n",
            "Elapsed time: Segment 4 13.46 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_4.pt\n",
            "Elapsed time: Segment 5 13.59 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_5.pt\n",
            "Elapsed time: Segment 6 14.80 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_6.pt\n",
            "Elapsed time: Segment 7 13.70 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_7.pt\n",
            "Elapsed time: Segment 8 14.13 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_8.pt\n",
            "Elapsed time: Segment 9 16.23 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_9.pt\n",
            "Elapsed time: Segment 10 14.12 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_10.pt\n",
            "Elapsed time: Segment 11 14.07 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_11.pt\n",
            "Elapsed time: Segment 12 14.40 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_12.pt\n",
            "Elapsed time: Segment 13 13.82 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_13.pt\n",
            "Elapsed time: Segment 14 14.11 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_14.pt\n",
            "Elapsed time: Segment 15 15.06 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_15.pt\n",
            "Elapsed time: Segment 16 13.89 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_16.pt\n",
            "Elapsed time: Segment 17 14.10 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_17.pt\n",
            "Elapsed time: Segment 18 16.25 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_18.pt\n",
            "Elapsed time: Segment 19 13.93 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_19.pt\n",
            "Elapsed time: Segment 20 13.95 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_20.pt\n",
            "Elapsed time: Segment 21 13.96 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_21.pt\n",
            "Elapsed time: Segment 22 14.08 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_22.pt\n",
            "Elapsed time: Segment 23 13.94 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_23.pt\n",
            "Elapsed time: Segment 24 13.70 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_24.pt\n",
            "Elapsed time: Segment 25 15.96 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_25.pt\n",
            "Elapsed time: Segment 26 14.19 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_26.pt\n",
            "Elapsed time: Segment 27 14.29 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_27.pt\n",
            "Elapsed time: Segment 28 14.16 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_28.pt\n",
            "Elapsed time: Segment 29 14.03 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_29.pt\n",
            "Elapsed time: Segment 30 14.05 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_30.pt\n",
            "Elapsed time: Segment 31 14.03 seconds\n",
            "Saved /content/drive/MyDrive/colab/data/Explosion002_x264.mp4_31.pt\n",
            "Elapsed time: 465.75 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define input tensor with shape [32, 151296]\n",
        "input_tensor = torch.randn(32, 151296)\n",
        "\n",
        "# Define a 1D convolutional layer\n",
        "# We need to set the number of input channels to 1, as we have a single feature sequence per sample\n",
        "# Output channels should be set to 2048 to match the desired output dimension\n",
        "# The kernel size and stride need to be determined to match the output size requirement\n",
        "class Conv1DLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv1DLayer, self).__init__()\n",
        "        # Define the convolutional layer\n",
        "        # in_channels = 1 (since we're treating each sequence as a single channel input)\n",
        "        # out_channels = 2048 (desired output features)\n",
        "        # kernel_size = (appropriate value, we'll calculate)\n",
        "        # stride = (appropriate value, we'll calculate)\n",
        "        # We'll use some padding to help adjust the size\n",
        "\n",
        "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=2048, kernel_size=73, stride=73)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add an extra dimension to match the input shape expected by Conv1d: (batch_size, in_channels, length)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv1d(x)\n",
        "        # Remove the extra dimension added earlier\n",
        "        x = x.squeeze(2)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "model = Conv1DLayer()\n",
        "\n",
        "# Apply the model to the input tensor\n",
        "output_tensor = model(input_tensor)\n",
        "\n",
        "print(\"Output tensor shape:\", output_tensor.shape)\n"
      ],
      "metadata": {
        "id": "bhy1NurFSc7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yziBaYfCD75m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3E4DmTEqDwlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define input tensor with shape [32, 151296]\n",
        "input_tensor = torch.randn(32, 151296)\n",
        "\n",
        "# Define a 1D convolutional layer\n",
        "class Conv1DLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv1DLayer, self).__init__()\n",
        "        # Define the convolutional layer\n",
        "        # in_channels = 1 (since we're treating each sequence as a single channel input)\n",
        "        # out_channels = 2048 (desired output features)\n",
        "        # kernel_size = appropriate value to achieve desired output\n",
        "        # stride = appropriate value to achieve desired output\n",
        "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=2048, kernel_size=73, stride=73)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add an extra dimension to match the input shape expected by Conv1d: (batch_size, in_channels, length)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv1d(x)\n",
        "        # Remove the extra dimension added earlier\n",
        "        x = x.squeeze(2)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "model = Conv1DLayer()\n",
        "\n",
        "# Apply the model to the input tensor\n",
        "output_tensor = model(input_tensor)\n",
        "\n",
        "print(\"Output tensor shape:\", output_tensor.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "tiP3xvR5UHmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define input tensor with shape [32, 151296]\n",
        "input_tensor = torch.randn(32, 151296)\n",
        "\n",
        "# Define an average pooling layer\n",
        "class AveragePoolingLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AveragePoolingLayer, self).__init__()\n",
        "        # Define the average pooling layer\n",
        "        # kernel_size and stride should be chosen to reduce the input dimension to the desired output dimension\n",
        "        self.avg_pool = nn.AvgPool1d(kernel_size=74, stride=74)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add an extra dimension to match the input shape expected by AvgPool1d: (batch_size, channels, length)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.avg_pool(x)\n",
        "        # Remove the extra dimension added earlier\n",
        "        x = x.squeeze(1)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "model = AveragePoolingLayer()\n",
        "\n",
        "# Apply the model to the input tensor\n",
        "output_tensor = model(input_tensor)\n",
        "\n",
        "print(\"Output tensor shape:\", output_tensor.shape)\n"
      ],
      "metadata": {
        "id": "SLHqNoL7UveR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "151296/2048"
      ],
      "metadata": {
        "id": "sJQ8nFVxVNHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_vector.shape"
      ],
      "metadata": {
        "id": "6Dcq62PcVm1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(frames)"
      ],
      "metadata": {
        "id": "FqlF9YwB6FVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "num_frames = math.floor(len(frames)/32)"
      ],
      "metadata": {
        "id": "-QVQnR2N6HrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_segments\n"
      ],
      "metadata": {
        "id": "yXPdm8rM6MlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "125*32"
      ],
      "metadata": {
        "id": "f3SmAMne6gzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "frames_split = np.array_split(frames, 32)"
      ],
      "metadata": {
        "id": "LUP_3pX66z68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames_split[22].shape"
      ],
      "metadata": {
        "id": "nbyZr7rs7Q0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-EopHr9J7YX2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}