{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1lD0r2dvLX11cUermJN1I4L3ZN2gOn5Rx",
      "authorship_tag": "ABX9TyOUuCaeOP9XglfFB8QJHOh3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthik111/video_anomaly_detection/blob/master/notebooks/VIT_Feature_Extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install decord"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G76Jew5cMpMZ",
        "outputId": "f92fb321-a289-4e56-b1e0-ecf3b18d6dac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: decord in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FGVvpP-5CayC"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTImageProcessor, ViTModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "url_c = 'https://farm4.staticflickr.com/3545/3409800178_24c6f790e6_z.jpg'\n",
        "image_c = Image.open(requests.get(url_c, stream=True).raw)\n",
        "\n",
        "url_d = 'https://farm6.staticflickr.com/5332/9374828651_07f9433075_z.jpg'\n",
        "image_d = Image.open(requests.get(url_d, stream=True).raw)\n",
        "\n",
        "url_d1 = 'https://farm3.staticflickr.com/2556/4228514131_81f3416db3_z.jpg'\n",
        "image_d1 = Image.open(requests.get(url_d1, stream=True).raw)\n",
        "\n",
        "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "#image = [np.random.randn(3, 224, 224) for _ in range(10)]\n",
        "\n",
        "inputs = processor(images=[image, image_c, image_d, image_d1], return_tensors=\"pt\")\n",
        "\n",
        "outputs = model(**inputs)\n",
        "last_hidden_states = outputs.last_hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['pixel_values'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBxpo50hCrMu",
        "outputId": "e4aa3c35-9f5f-4e72-bfd7-5204e9d4a70c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_states.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrp-jsTkDDPH",
        "outputId": "f789fb15-1a89-4b91-dfc4-f5c266814b5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "ffkfSQetDRDn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = ['/content/drive/MyDrive/colab/data/kart.mov',\n",
        "              '/content/drive/MyDrive/colab/data/Assault007_x264.mp4',\n",
        "              '/content/drive/MyDrive/colab/data/Explosion002_x264.mp4']\n"
      ],
      "metadata": {
        "id": "ZAeI0b8lJ9F-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import decord\n",
        "vr = decord.VideoReader(video_path[2])"
      ],
      "metadata": {
        "id": "8sMA5kI3EzqN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vr.get_avg_fps()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d48m1tUhMk3E",
        "outputId": "14b36630-e8f0-4503-ae28-2d28a0d0bac1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30.0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total number of frames in the video\n",
        "num_frames = len(vr)\n",
        "\n",
        "# Read all frames\n",
        "frames = [vr[i].asnumpy() for i in range(num_frames)]\n",
        "\n",
        "# Display some information\n",
        "print(f\"Total number of frames: {num_frames}\")\n",
        "print(f\"Shape of a single frame: {frames[0].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7CS0-fdMlxL",
        "outputId": "dd640d5c-12a9-42a3-daa9-0661428a8eac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of frames: 4013\n",
            "Shape of a single frame: (240, 320, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(images=frames[:32], return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "YDpDcxujE-qv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['pixel_values'].shape"
      ],
      "metadata": {
        "id": "5KB4fEX6FAQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bab5314-9c31-4447-d2fe-8c8a89220cbf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "last_hidden_states = outputs.last_hidden_state"
      ],
      "metadata": {
        "id": "Fn5UJuh2OKWd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_states.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYg2x2osOcj5",
        "outputId": "1398294a-9254-46fc-b6e5-e911aa80a252"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: flatten above [32,197,168] vector to [32,192*168] vector\n",
        "\n",
        "flattened_vector = last_hidden_states.reshape(last_hidden_states.shape[0], -1)\n",
        "flattened_vector.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgQnRylcO4iE",
        "outputId": "c9311432-a81e-450e-a404-c0676717b0f3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 151296])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: apply convolution to convert a [32,151296]dimension feature tensor to a [32,2048] feature tensor\n",
        "\n",
        "import torch.nn as nn\n",
        "conv = nn.Conv1d(in_channels=151296, out_channels=2048, kernel_size=768)\n",
        "flattened_vector = conv(flattened_vector)\n",
        "flattened_vector.shape\n"
      ],
      "metadata": {
        "id": "PR4EB6PaSGJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define input tensor with shape [32, 151296]\n",
        "input_tensor = torch.randn(32, 151296)\n",
        "\n",
        "# Define a 1D convolutional layer\n",
        "# We need to set the number of input channels to 1, as we have a single feature sequence per sample\n",
        "# Output channels should be set to 2048 to match the desired output dimension\n",
        "# The kernel size and stride need to be determined to match the output size requirement\n",
        "class Conv1DLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv1DLayer, self).__init__()\n",
        "        # Define the convolutional layer\n",
        "        # in_channels = 1 (since we're treating each sequence as a single channel input)\n",
        "        # out_channels = 2048 (desired output features)\n",
        "        # kernel_size = (appropriate value, we'll calculate)\n",
        "        # stride = (appropriate value, we'll calculate)\n",
        "        # We'll use some padding to help adjust the size\n",
        "\n",
        "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=2048, kernel_size=73, stride=73)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add an extra dimension to match the input shape expected by Conv1d: (batch_size, in_channels, length)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv1d(x)\n",
        "        # Remove the extra dimension added earlier\n",
        "        x = x.squeeze(2)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "model = Conv1DLayer()\n",
        "\n",
        "# Apply the model to the input tensor\n",
        "output_tensor = model(input_tensor)\n",
        "\n",
        "print(\"Output tensor shape:\", output_tensor.shape)\n"
      ],
      "metadata": {
        "id": "bhy1NurFSc7c",
        "outputId": "ad9797ad-e3ee-4137-948d-6698cb9f3a51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output tensor shape: torch.Size([32, 2048, 2072])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define input tensor with shape [32, 151296]\n",
        "input_tensor = torch.randn(32, 151296)\n",
        "\n",
        "# Define a 1D convolutional layer\n",
        "class Conv1DLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv1DLayer, self).__init__()\n",
        "        # Define the convolutional layer\n",
        "        # in_channels = 1 (since we're treating each sequence as a single channel input)\n",
        "        # out_channels = 2048 (desired output features)\n",
        "        # kernel_size = appropriate value to achieve desired output\n",
        "        # stride = appropriate value to achieve desired output\n",
        "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=2048, kernel_size=73, stride=73)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add an extra dimension to match the input shape expected by Conv1d: (batch_size, in_channels, length)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.conv1d(x)\n",
        "        # Remove the extra dimension added earlier\n",
        "        x = x.squeeze(2)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "model = Conv1DLayer()\n",
        "\n",
        "# Apply the model to the input tensor\n",
        "output_tensor = model(input_tensor)\n",
        "\n",
        "print(\"Output tensor shape:\", output_tensor.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "tiP3xvR5UHmB",
        "outputId": "03e272da-a67a-470e-a3bd-c89f147e1b41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output tensor shape: torch.Size([32, 2048, 2072])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define input tensor with shape [32, 151296]\n",
        "input_tensor = torch.randn(32, 151296)\n",
        "\n",
        "# Define an average pooling layer\n",
        "class AveragePoolingLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AveragePoolingLayer, self).__init__()\n",
        "        # Define the average pooling layer\n",
        "        # kernel_size and stride should be chosen to reduce the input dimension to the desired output dimension\n",
        "        self.avg_pool = nn.AvgPool1d(kernel_size=74, stride=74)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add an extra dimension to match the input shape expected by AvgPool1d: (batch_size, channels, length)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.avg_pool(x)\n",
        "        # Remove the extra dimension added earlier\n",
        "        x = x.squeeze(1)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "model = AveragePoolingLayer()\n",
        "\n",
        "# Apply the model to the input tensor\n",
        "output_tensor = model(input_tensor)\n",
        "\n",
        "print(\"Output tensor shape:\", output_tensor.shape)\n"
      ],
      "metadata": {
        "id": "SLHqNoL7UveR",
        "outputId": "8c36b131-b629-4543-875c-837daee70c81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output tensor shape: torch.Size([32, 2044])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "151296/2048"
      ],
      "metadata": {
        "id": "sJQ8nFVxVNHy",
        "outputId": "7a35c8f6-1264-49ff-f52e-e138db32e56c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73.875"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Dcq62PcVm1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}